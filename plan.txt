================================================================================
ETHEREUM DATA PIPELINE OPTIMIZATION PLAN
================================================================================

CURRENT STATE
-------------
- Processing Rate: 5,000-10,000 tx/s
- Tech Stack: Python + asyncio + PostgreSQL + simdjson + COPY
- Optimizations Already Implemented:
  * simdjson (2-10x faster JSON parsing)
  * Direct tuples instead of dataclasses (12-44% faster)
  * COPY for bulk inserts (10-100x faster than INSERT)
  * Parallel RPC calls (3x getLogs concurrent)
  * Parallel DB inserts (ERC20 + NFT)
  * Async I/O with aiohttp/asyncpg

BOTTLENECKS IDENTIFIED
----------------------
- RPC: 5,550-23,788 tx/s (network I/O)
- proc: 4,573-9,445 tx/s (BOTTLENECK - overall processing)
- DB: 52,126-78,020 tx/s (PostgreSQL performing well)

Target: Improve proc throughput to 50,000-100,000 tx/s


================================================================================
PHASE 1: LOW-HANGING FRUIT (1-2 days effort)
================================================================================

1.1 BENCHMARK TESTS
-------------------
Goal: Measure pure parsing performance without DB overhead
Effort: 2-3 hours
Expected gain: Baseline measurement, not an optimization

Tasks:
- Create benchmark script that fetches real Erigon logs
- Parse to tuples without DB insertion
- Measure parsing throughput (tx/s)
- Test with different batch sizes
- Identify parsing bottlenecks with cProfile

Deliverable: benchmark_parsing.py with performance metrics


1.2 CSV COPY FORMAT
-------------------
Goal: Faster DB inserts by using CSV format instead of tuple format
Effort: 3-4 hours
Expected gain: 10-20% faster DB inserts

Current approach:
  transfer_data = [(block_num, tx_hash, value, ...)]
  COPY with tuple format

New approach:
  csv_data = "block_num,tx_hash,value\n..."
  COPY with CSV format (PostgreSQL just splits on comma, no parsing)

Implementation:
- Modify insert_transfers() to generate CSV string
- Use COPY ... FROM STDIN WITH CSV
- Benchmark improvement

Trade-off: Minimal code change, solid improvement


1.3 PROFILING WITH CPROFILE
----------------------------
Goal: Find actual bottlenecks in production code
Effort: 2-3 hours
Expected gain: Identify optimization targets

Tasks:
- Run 5.PY with cProfile on representative workload
- Generate flame graph or call statistics
- Identify top 10 slowest functions
- Focus optimization efforts on hot paths

Tools:
- python -m cProfile -o profile.stats 5.PY --iteration 100
- snakeviz profile.stats (visualization)
- py-spy for sampling profiler (no overhead)

Deliverable: Profile report showing hot paths


================================================================================
PHASE 2: MEDIUM EFFORT OPTIMIZATIONS (1 week)
================================================================================

2.1 PREFETCHING / PIPELINE PARALLELISM
---------------------------------------
Goal: Overlap fetch, parse, and DB insert stages
Effort: 1-2 days
Expected gain: 20-30% improvement

Current flow (sequential):
  Batch 1: [Fetch] -> [Parse] -> [DB Insert]
  Batch 2:                        [Fetch] -> [Parse] -> [DB Insert]

New flow (pipelined):
  Batch 1: [Fetch] -> [Parse] -> [DB Insert]
  Batch 2:            [Fetch] -> [Parse] -> [DB Insert]
  Batch 3:                       [Fetch] -> [Parse] -> [DB Insert]

Implementation:
- Use asyncio.Queue for stage communication
- Producer task: Fetch logs and push to parse queue
- Parser task: Parse logs and push to DB queue
- DB task: Insert to database
- Keeps all stages busy simultaneously

Benefits:
- Network I/O doesn't block parsing
- Parsing doesn't block DB inserts
- Better CPU/network utilization

Trade-off: More complex code, higher memory usage


2.2 INCREASE BATCH SIZE
------------------------
Goal: Reduce per-batch overhead
Effort: 2 hours (testing)
Expected gain: 10-30% improvement if Erigon allows

Current: 100 blocks per batch
Test: 500, 1000, 2000 blocks per batch

Benefits:
- Fewer RPC calls
- Fewer DB transactions
- Less Python overhead per batch

Risks:
- Higher memory usage
- Erigon may timeout on large requests
- Longer individual batch time (worse for monitoring)

Testing plan:
- Gradually increase batch size
- Monitor RPC response time
- Find optimal balance (likely 500-1000 blocks)


2.3 TABLE PARTITIONING
-----------------------
Goal: 10-100x faster queries, easier maintenance
Effort: 4-6 hours (with downtime) OR 1-2 weeks (zero downtime)
Expected gain: Massive query speedup, no insert improvement

Strategy: Partition by block_number (range partitioning)

Partition layout:
  erc20_transfers_0         (blocks 0-999,999)
  erc20_transfers_1000000   (blocks 1M-1,999,999)
  erc20_transfers_2000000   (blocks 2M-2,999,999)
  ... (~25 partitions total)

Benefits:
- Queries scan only relevant partitions (10-100x faster)
- Smaller indexes per partition (less memory)
- Fast VACUUM per partition (minutes vs hours)
- Easy archival (drop old partitions instantly)
- Parallel query execution across partitions

Implementation (with downtime):
1. Rename existing tables (backup)
2. Create partitioned parent tables:

   CREATE TABLE erc20_transfers (
     id BIGSERIAL,
     block_number BIGINT NOT NULL,
     ...
   ) PARTITION BY RANGE (block_number);

3. Create all partitions:

   CREATE TABLE erc20_transfers_0 PARTITION OF erc20_transfers
   FOR VALUES FROM (0) TO (1000000);

   CREATE TABLE erc20_transfers_1000000 PARTITION OF erc20_transfers
   FOR VALUES FROM (1000000) TO (2000000);

   ... (repeat for all ranges)

4. Copy data from old tables to partitions
5. Recreate indexes on each partition
6. Update application code to auto-create partitions:

   async def ensure_partition_exists(block_number):
       partition_start = (block_number // 1000000) * 1000000
       partition_end = partition_start + 1000000
       CREATE TABLE IF NOT EXISTS erc20_transfers_{partition_start}
       PARTITION OF erc20_transfers
       FOR VALUES FROM ({partition_start}) TO ({partition_end})

7. Drop old tables

Alternative (zero downtime):
1. Create partitioned tables alongside existing
2. Dual-write to both tables
3. Background copy historical data
4. Switch reads when ready
5. Drop old tables

Considerations:
- UNIQUE constraints must include partition key
  Change: UNIQUE(transaction_hash, log_index)
  To: UNIQUE(transaction_hash, log_index, block_number)

- COPY to partitions is 5-10% slower (routing overhead)
  Mitigation: COPY directly to specific partition

Maintenance:
- Pre-create future partitions before reaching boundary
- Weekly VACUUM per partition
- Monthly archival of old partitions
- Use pg_partman extension for automation

Performance example:
  Query: SELECT * FROM erc20_transfers WHERE block_number BETWEEN 5M-6M

  Before: Seq Scan 20M rows, 45 seconds
  After:  Seq Scan 1M rows (single partition), 150ms

  Result: 300x faster


================================================================================
PHASE 3: MAJOR CHANGES (2-3 weeks)
================================================================================

3.1 GRPC MIGRATION
------------------
Goal: 30-50% faster RPC calls, lower latency
Effort: 8-10 hours
Expected gain: 30-50% overall improvement

Current: JSON-RPC over HTTP
  Python -> HTTP POST -> JSON -> Erigon
  Erigon -> JSON -> HTTP -> simdjson parse -> Python

Target: gRPC with Protocol Buffers
  Python -> gRPC -> Protobuf binary -> Erigon
  Erigon -> Protobuf binary -> gRPC -> Python (zero-copy)

Benefits:
- Binary format (no JSON serialization)
- HTTP/2 multiplexing (better connection usage)
- Protobuf parsing faster than JSON
- Persistent connections (lower latency)
- Header compression
- Smaller payload size

Implementation plan:

Step 1: Setup Erigon gRPC (2-3 hours)
- Enable Erigon remote API: --private.api.addr=localhost:9090
- Get Erigon .proto files (protocol buffer definitions)
- Install: pip install grpcio grpcio-tools
- Generate Python stubs: python -m grpc_tools.protoc --proto_path=. --python_out=. --grpc_python_out=. erigon.proto

Step 2: Create gRPC Client (3-4 hours)
- Implement ErigonGrpcClient class
- Methods:
  * GetBlockByNumber() - replace eth_getBlockByNumber
  * GetLogs() - replace eth_getLogs (if available)
  * GetLatestBlock() - replace eth_blockNumber
- Connection pooling (gRPC has built-in channel pooling)
- Error handling and retries
- Streaming support for large responses

Step 3: Parallel Testing (2 hours)
- Run JSON-RPC and gRPC side-by-side
- Compare throughput, latency, CPU usage, memory
- Verify data consistency
- Benchmark with --iteration tests

Step 4: Production Migration (1 hour)
- Switch to gRPC in production
- Remove JSON-RPC code
- Update metrics/logging

Erigon gRPC API services:
- remote.ETHBACKEND: Etherbase, NetVersion, NetPeerCount
- remote.KV: Direct database access (Get, Range, Snapshots)
- web3.TXPOOL: Transaction pool operations

IMPORTANT NOTE:
Erigon's gRPC API may not have exact eth_getLogs equivalent.
Options:
1. Use Erigon's KV service to query logs table directly (fastest)
2. Keep JSON-RPC for getLogs, use gRPC for blocks (hybrid)
3. Use Erigon's custom RPC extensions if available

Recommended: Hybrid approach
- gRPC for block metadata (faster)
- Keep JSON-RPC for getLogs (widely supported)
- Migrate getLogs to gRPC if/when available

Expected results:
- Latency: 20-40% reduction per request
- Throughput: 30-50% improvement
- CPU usage: 20-30% reduction (less parsing)
- Memory: 10-20% reduction (binary more compact)

Trade-offs:
- More complex setup
- Dependency on Erigon's gRPC API stability
- Less portable (JSON-RPC is standard)


3.2 MULTI-PROCESS WORKERS
--------------------------
Goal: Linear scaling with CPU cores
Effort: 2-3 days
Expected gain: Near-linear with core count (4 cores = 4x throughput)

Current: Single Python process (limited by GIL)
Target: Multiple worker processes, each handling different block ranges

Architecture:

Coordinator process:
- Divides block range into chunks
- Assigns chunks to workers
- Monitors progress
- Handles failures

Worker processes (one per CPU core):
- Worker 1: Blocks 0-1M
- Worker 2: Blocks 1M-2M
- Worker 3: Blocks 2M-3M
- Worker 4: Blocks 3M-4M

Each worker:
- Independent RPC client
- Independent DB connection
- Processes assigned range
- Reports progress to coordinator

Implementation:

import multiprocessing

def worker_process(worker_id, start_block, end_block):
    # Create ERC20Extractor instance
    # Process blocks start_block to end_block
    # Independent of other workers

def main():
    num_workers = multiprocessing.cpu_count()
    total_blocks = latest_block - start_block
    chunk_size = total_blocks // num_workers

    processes = []
    for i in range(num_workers):
        start = start_block + (i * chunk_size)
        end = start + chunk_size - 1
        p = multiprocessing.Process(target=worker_process, args=(i, start, end))
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

Benefits:
- Bypasses Python GIL (each process has own interpreter)
- Linear scaling with cores (4 cores = 4x throughput)
- Fault isolation (one worker crash doesn't affect others)
- Can prioritize recent blocks (one worker on latest, others on historical)

Considerations:
- PostgreSQL connection limit (max_connections setting)
- Erigon RPC rate limiting
- Coordination overhead
- Memory usage (each process loads full codebase)

Trade-offs:
- More complex coordination
- Need to track which worker owns which blocks
- Harder to debug
- Higher memory usage


3.3 CYTHON FOR HOT PATHS (IF NEEDED)
-------------------------------------
Goal: 5-10x faster parsing for critical functions
Effort: 3-5 days
Expected gain: 5-10x for hot paths, 20-50% overall

Only implement if profiling shows Python is bottleneck.

Hot paths to optimize:
- hex_to_bytes() - called millions of times
- hex_to_decimal() - Decimal conversion
- parse_erc20_transfer() - main parsing logic

Implementation:
1. Create .pyx files for hot functions
2. Add type annotations:

   cdef bytes hex_to_bytes_fast(str hex_string):
       cdef bytes result
       # C-level implementation
       return result

3. Compile with Cython
4. Replace Python imports with Cython versions
5. Benchmark improvement

Expected results:
- hex_to_bytes: 10-50x faster
- hex_to_decimal: 5-20x faster (limited by Decimal object creation)
- Overall: 20-50% improvement if parsing is bottleneck

Trade-offs:
- Compilation step (more complex deployment)
- Less readable code
- Debugging harder
- Only helps CPU-bound operations (not I/O)

Alternative: Consider Rust with PyO3 instead
- Faster than Cython (50-100x for some operations)
- Better type safety
- Modern tooling
- Harder learning curve


================================================================================
ESTIMATED PERFORMANCE GAINS
================================================================================

Baseline (current):                    5,000-10,000 tx/s

After Phase 1 (benchmarking + CSV):    6,000-12,000 tx/s  (+20%)
  - CSV COPY: +10-20%
  - Profiling identifies further optimizations

After Phase 2 (prefetch + partition):  15,000-30,000 tx/s (+150%)
  - Prefetching: +20-30%
  - Larger batches: +10-30%
  - Partitioning: No insert improvement, massive query improvement

After Phase 3 (gRPC + workers):        50,000-100,000 tx/s (+500%)
  - gRPC: +30-50%
  - Multi-process (4 cores): +300% (4x)
  - Cython (if needed): +20-50%

Combined effect: 10-20x improvement possible


================================================================================
IMPLEMENTATION ORDER (RECOMMENDED)
================================================================================

Week 1: Measurement & Quick Wins
  Day 1-2: Benchmark tests + profiling
  Day 3-4: CSV COPY format implementation
  Day 5:   Analyze results, plan next steps

Week 2: Prefetching & Batch Optimization
  Day 1-3: Implement pipeline parallelism / prefetching
  Day 4-5: Test batch size increases, tune parameters

Week 3: Table Partitioning
  Day 1-2: Plan partitioning strategy, test migration
  Day 3-4: Execute migration (with downtime)
  Day 5:   Verify queries, update monitoring

Week 4: gRPC Migration
  Day 1-2: Setup gRPC, generate stubs, implement client
  Day 3:   Parallel testing (JSON-RPC vs gRPC)
  Day 4:   Production migration
  Day 5:   Monitor and tune

Week 5-6: Multi-Process Workers (optional)
  Day 1-3: Implement worker coordination
  Day 4-5: Test and tune

Future: Cython optimization (only if profiling shows need)


================================================================================
RISKS & MITIGATION
================================================================================

Risk: PyPy incompatibility with asyncpg/simdjson
Mitigation: Skip PyPy, not worth losing simdjson gains

Risk: Erigon rate limiting with parallel workers
Mitigation: Implement backoff, limit concurrent requests

Risk: PostgreSQL connection exhaustion
Mitigation: Increase max_connections, use connection pooling

Risk: Partitioning migration downtime
Mitigation: Use zero-downtime migration with dual-write

Risk: gRPC API changes in Erigon updates
Mitigation: Version lock Erigon, thorough testing before upgrade

Risk: Complex debugging with multi-process
Mitigation: Extensive logging, worker health checks

Risk: Over-optimization (effort > gain)
Mitigation: Profile first, measure everything, stop when good enough


================================================================================
MONITORING & METRICS
================================================================================

Current metrics (keep):
- span: Time range of blocks
- RPC: tx/s from Erigon
- proc: tx/s overall processing
- DB: tx/s database insertion

Add for production:
- Latency percentiles (p50, p95, p99)
- Error rate per stage
- Queue depths (if using pipeline)
- Worker health (if using multi-process)
- PostgreSQL metrics (connections, lock waits, cache hit ratio)
- Memory usage per stage
- Blocks behind latest (data freshness)

Tools:
- Prometheus + Grafana for metrics
- pg_stat_statements for query analysis
- Custom dashboard showing pipeline health


================================================================================
SUCCESS CRITERIA
================================================================================

Phase 1 Success:
- Baseline benchmarks established
- 20% improvement from CSV COPY
- Profile identifies top 3 bottlenecks

Phase 2 Success:
- 150% overall improvement
- Prefetching reduces idle time
- Partitioning enables fast historical queries (<1s for any block range)

Phase 3 Success:
- 500% overall improvement (50k-100k tx/s)
- gRPC reduces RPC time by 30%
- Multi-process scales linearly with cores
- System handles Ethereum mainnet sync at block production rate


================================================================================
MAINTENANCE PLAN
================================================================================

Daily:
- Monitor error rates
- Check failed block retries
- Verify workers are healthy (if multi-process)

Weekly:
- VACUUM each partition
- Check partition sizes
- Review slow query log
- Update statistics

Monthly:
- Archive old partitions
- Review and optimize slow queries
- Check index usage
- Update dependencies

Quarterly:
- Review partitioning strategy (adjust size if needed)
- Performance regression testing
- Disaster recovery drill


================================================================================
COST ANALYSIS
================================================================================

Development time:
- Phase 1: 2 days
- Phase 2: 1 week
- Phase 3: 2-3 weeks
Total: ~4-5 weeks engineering time

Infrastructure costs:
- No additional cost for optimizations
- Potential savings from faster processing (less compute time)
- Partitioning may reduce storage costs (easier archival)

ROI:
- 10-20x throughput improvement
- Faster historical queries (better user experience)
- Easier maintenance (partitioning)
- Better observability (metrics)


================================================================================
CONCLUSION
================================================================================

This plan provides a roadmap to improve processing throughput from 5-10k tx/s
to 50-100k tx/s through systematic optimizations. Each phase builds on the
previous one, with clear goals and measurable outcomes.

Start with Phase 1 to establish baselines and quick wins, then proceed to
Phase 2 for substantial improvements, and Phase 3 for maximum performance.

Profile first, measure everything, and optimize based on data, not assumptions.
